{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine import *\n",
    "from keras.engine.topology import Container\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import *\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from functools import reduce\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from matplotlib import pylab as plt\n",
    "from IPython.display import clear_output\n",
    "from MeCab import Tagger\n",
    "from prefetch_generator import BackgroundGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mecaber = Tagger(\"-Ochasen\")\n",
    "with open(\"./timeline/chahan69.txt\", \"r\") as f:\n",
    "    texts = []\n",
    "    for line in f.readlines():\n",
    "        pos = mecaber.parse(line).split(\"\\n\")[:-1]\n",
    "        if len(pos) < 7:\n",
    "            continue\n",
    "        words = [\"sosos\"] + [x.split(\"\\t\")[0] for x in pos]\n",
    "        texts.append(\" \".join(words))\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(texts)\n",
    "VOCAB = len(tokenizer.word_index)\n",
    "X = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InvTokenizer:\n",
    "    def __init__(self, dic):\n",
    "        self.dic = dic\n",
    "        self.num = len(dic)\n",
    "        self.invdec = [\"\" for _ in range(self.num + 1)]\n",
    "        for c, i in self.dic.items():\n",
    "            self.invdec[i] = c\n",
    "    \n",
    "    def __call__(self, array):\n",
    "        if len(array) == 0: return(\"\")\n",
    "        return(reduce(lambda x,y:x+y,[self.invdec[int(i)] for i in list(array)]))\n",
    "            \n",
    "texgen = InvTokenizer(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inputs = Input(shape=(None,))\n",
    "    y = inputs\n",
    "    y = Embedding(VOCAB+1, 512, mask_zero=True)(y)\n",
    "#     y = GRU(512, return_sequences=True, dropout=0.2)(y)\n",
    "    y = GRU(512, return_sequences=True, dropout=0.2)(y)\n",
    "    y = GRU(512)(y)\n",
    "    y = Dense(VOCAB+1, activation=\"softmax\")(y)\n",
    "    model = Model(inputs, y)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=RMSprop(1e-3))\n",
    "    return(model)\n",
    "\n",
    "def datagen(X, batch_size=128):\n",
    "    x, y = [], []\n",
    "    np.random.shuffle(X)\n",
    "    for text in tqdm_notebook(X):\n",
    "        for i in range(2, len(text)):\n",
    "            x.append(text[max(0, i-TEXT_LENGTH):i])\n",
    "            y.append(text[i])\n",
    "        if len(y) >= batch_size:\n",
    "            yield(pad_sequences(np.array(x), maxlen=TEXT_LENGTH, padding=\"post\"), np.array(y))\n",
    "            x, y = [], []\n",
    "\n",
    "def get_text(mod):\n",
    "    x = [[tokenizer.word_index[\"sosos\"]]]\n",
    "    while len(x[0]) < 50:\n",
    "        y = mod.predict(np.array(x))[0]\n",
    "        y = np.random.choice(VOCAB+1, p=y)\n",
    "        if y == 1:\n",
    "            return(texgen(x[0][1:]))\n",
    "        x[0].append(y)\n",
    "    return(texgen(x[0][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod = get_model()\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "batch_size = 512\n",
    "mod.optimizer = RMSprop(1e-5)\n",
    "for e in range(epoch):\n",
    "    gen = datagen(X, batch_size=batch_size)\n",
    "    for x, y in BackgroundGenerator(gen):\n",
    "        loss = mod.train_on_batch(x, y)\n",
    "        history.append(loss)\n",
    "        clear_output()\n",
    "        plt.plot(history)\n",
    "        plt.show()\n",
    "        print(get_text(mod))\n",
    "mod.save(filepath=\"./auto_twitter.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自信すぎ！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
      "松屋の終わりだけどので赤書く人って言ってる\n",
      "ぬくなってねぇだろ´･･\n",
      "dアニメストアを淫乱につくやつを教えてほしい\n",
      "週間買ったりよーてほしい\n",
      "理っあんまりオーラ10億ひいたwwo\n",
      "ァ！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
      "きょう登録した最強の目先ﾃ2曲くなり傾いられるようにしたんだよなね\n",
      "ニーダーの話ですか？\n",
      "わかりません今もショック\n",
      "ダブルラリアット人ワァ～もらえる反応しないでエアてんdp問題集やろ\n",
      "犬無限にいつか凄い\n",
      "うまく館が都知事たのはすきなんだけど\n",
      "もり本人はいったあすo丿\n",
      "戻してとれませんか！？\n",
      "すくなくなったの今だけちょい\n",
      "遊び呆ける！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
      "～～～～～～～～～～～～～がる絶対書くの？？？？？？？\n",
      "晴れ食ったんコースらい語源500000000020カフェイン取った\n",
      "なくcookieたん激か迷う\n",
      "生きていきます！！！\n",
      "蛇の目の風後のためおやすみしそう\n",
      "いちゃいちゃされたマジィいっぱいベスト\n",
      "書道のbpに考えてほしいな\n",
      "、pmおいしいこと京都大学総合研究7号館旧工学部10号館in京都市京都府\n",
      "常識と50年話だった\n",
      "pmthunderstormstomorrowwithahighof4candalowof人c\n",
      "れん！？ーーーー！\n",
      "ふぁ既にな～すぎる雷減らすでかて変換inkyoto京都府\n",
      "ンモー。。僕は。。\n",
      "ゲキと真剣演習書いたので作っていい\n",
      "値下げしたりなきゃかな\n",
      "お金違いた富山勝手にしてホームページ\n",
      "なあ～絶対アイスあるやんけこれ\n",
      "応用69違いばいいんですか？強いので次起きない？？寝るんか\n",
      "勝手に倍前になってみてしまって200ゲージ落ちたようなものあるんかな\n",
      "最c池田屋のゲームビンゴであいの命名された解釈りぁぁぁぁぁぁぁぁぁ3つに館が強いようにうた\n",
      "名前学習があるんだば、bmsなんです、ウデマエ倍率スパブロしてしまっなの、･｡な\n",
      "陸過ぎるちなんじゃない！？\n",
      "全員目使わない持つかまわない？？\n",
      "工場高いから行こうように報酬別に海外でさっぱり！！！！\n",
      "3000円が研いくやつぐらいやめてて深ねぇ\n",
      "てんことですか？？\n",
      "それは酒だけですか……たのしい´･･！3000抜き私とtomorrowいきとたらやな。。。\n",
      "いえば死んになりそうですそんな〜〜〜〜〜〜〜〜〜のわかりになってくれた\n",
      "ssrsim！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
      "障害の進捗ですか？\n",
      "鬼は極刑したいちゃん…\n",
      "faqに割れる冷静にはきえねえな´･･\n",
      "だらけでしまいそうなのかな\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([get_text(mod) for _ in range(50)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "0f47f44fbd1b4bc1b9a8ae38c8cf1860": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "2c92e6dd07c64312afc93e0c78178b32": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "57ba01bbc9e74802ad4300fd2678422c": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "77b54b9f22f248a9baf4cd5cbb0f6642": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "788f019e30a247f492b6a0da6020650b": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "9605f54018194241ad2372471b01fbf1": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "b4d3d713b5894d52a9fea60ab4f50178": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
